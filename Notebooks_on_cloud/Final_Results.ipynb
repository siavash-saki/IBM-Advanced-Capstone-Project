{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='7ef2bc82-f627-4265-be50-c3f2215d4b5d', project_access_token='p-5c21f16b313f85f77558be65262bddeac2431407')\n",
    "pc = project.project_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Prediction Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have evaluated our model, we can use all the data and build a model to predict values of the future. In this case, we predict Electricity Consumption and Generation in year 2020 in Germany."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from matplotlib.dates import date2num\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input, LSTM\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-06 14:09:19--  https://github.com/siavash-saki/IBM-Advanced-Capstone-Project/raw/master/Data_Cleaned/consumption_ready_for_forcast.pkl\n",
      "Resolving github.com (github.com)... 140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/siavash-saki/IBM-Advanced-Capstone-Project/master/Data_Cleaned/consumption_ready_for_forcast.pkl [following]\n",
      "--2020-02-06 14:09:19--  https://raw.githubusercontent.com/siavash-saki/IBM-Advanced-Capstone-Project/master/Data_Cleaned/consumption_ready_for_forcast.pkl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.60.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.60.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 702204 (686K) [application/octet-stream]\n",
      "Saving to: ‘consumption_ready_for_forcast.pkl.2’\n",
      "\n",
      "100%[======================================>] 702,204     --.-K/s   in 0.04s   \n",
      "\n",
      "2020-02-06 14:09:19 (16.2 MB/s) - ‘consumption_ready_for_forcast.pkl.2’ saved [702204/702204]\n",
      "\n",
      "--2020-02-06 14:09:21--  https://github.com/siavash-saki/IBM-Advanced-Capstone-Project/raw/master/Data_Cleaned/generation_ready_for_forcast.pkl\n",
      "Resolving github.com (github.com)... 140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/siavash-saki/IBM-Advanced-Capstone-Project/master/Data_Cleaned/generation_ready_for_forcast.pkl [following]\n",
      "--2020-02-06 14:09:21--  https://raw.githubusercontent.com/siavash-saki/IBM-Advanced-Capstone-Project/master/Data_Cleaned/generation_ready_for_forcast.pkl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.60.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.60.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 702188 (686K) [application/octet-stream]\n",
      "Saving to: ‘generation_ready_for_forcast.pkl.2’\n",
      "\n",
      "100%[======================================>] 702,188     --.-K/s   in 0.02s   \n",
      "\n",
      "2020-02-06 14:09:21 (37.3 MB/s) - ‘generation_ready_for_forcast.pkl.2’ saved [702188/702188]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/siavash-saki/IBM-Advanced-Capstone-Project/raw/master/Data_Cleaned/consumption_ready_for_forcast.pkl\n",
    "!wget https://github.com/siavash-saki/IBM-Advanced-Capstone-Project/raw/master/Data_Cleaned/generation_ready_for_forcast.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption= pd.read_pickle('consumption_ready_for_forcast.pkl')\n",
    "generation= pd.read_pickle('generation_ready_for_forcast.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size and Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the batch size and timesteps\n",
    "batch_size = 128\n",
    "timesteps=24*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate training size regarding batch_size\n",
    "def get_train_length(dataset, timesteps, batch_size):\n",
    "    # substract test_percent to be excluded from training, reserved for testset\n",
    "    length = len(dataset)-2.1*timesteps\n",
    "    train_length_values = []\n",
    "    for x in range(int(length) - 1000,int(length)): \n",
    "        modulo=x%batch_size\n",
    "        if (modulo == 0):\n",
    "            train_length_values.append(x)\n",
    "    return (max(train_length_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets length: 43824\n",
      "Last divisible index: 43728\n",
      "Train Sets length: 43392 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = get_train_length(consumption, timesteps, batch_size)\n",
    "upper_train = length + timesteps*2\n",
    "print('\\nDatasets length:',len(consumption))\n",
    "print('Last divisible index:', upper_train)\n",
    "print('Train Sets length:', length,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Sets Shapes after Adding Timesteps: (43728, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set y_train variable for consumption df\n",
    "consumption_train_df = consumption[0:upper_train]\n",
    "consumption_y_train = consumption_train_df.iloc[:,].values\n",
    "print('\\nTraining Sets Shapes after Adding Timesteps:', consumption_y_train.shape)\n",
    "\n",
    "# Set y_train variable for generation df\n",
    "generation_train_df = generation[0:upper_train]\n",
    "generation_y_train = generation_train_df.iloc[:,].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale between 0 and 1. the weights are esier to find.\n",
    "sc_con = MinMaxScaler(feature_range = (0, 1))\n",
    "sc_gen = MinMaxScaler(feature_range = (0, 1))\n",
    "consumption_y_train_scaled = sc_con.fit_transform(np.float64(consumption_y_train))\n",
    "generation_y_train_scaled = sc_gen.fit_transform(np.float64(generation_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data structure with n timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Lists to store X_train and y_train\n",
    "consumption_X_train_matrix = []\n",
    "consumption_y_train_matrix = []\n",
    "# Creating a data structure with n timesteps\n",
    "for i in range(timesteps, length + timesteps):\n",
    "    #create X_train matrix\n",
    "    #24*7 items per array (timestep) \n",
    "    consumption_X_train_matrix.append(consumption_y_train_scaled[i-timesteps:i,0])\n",
    "    #create Y_train matrix\n",
    "    #24*7 items per array (timestep)\n",
    "    consumption_y_train_matrix.append(consumption_y_train_scaled[i:i+timesteps,0])\n",
    "    \n",
    "# reapeat all of these steps fot generation dataframe\n",
    "generation_X_train_matrix = []\n",
    "generation_y_train_matrix = []\n",
    "for i in range(timesteps, length + timesteps):\n",
    "    generation_X_train_matrix.append(generation_y_train_scaled[i-timesteps:i,0])\n",
    "    generation_y_train_matrix.append(generation_y_train_scaled[i:i+timesteps,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train sets shape: (43392, 168)\n",
      "y_train sets shape: (43392, 168)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check shape\n",
    "print()\n",
    "print('X_train sets shape:', np.array(consumption_X_train_matrix).shape)\n",
    "print('y_train sets shape:', np.array(consumption_y_train_matrix).shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn list into numpy array\n",
    "consumption_X_train_matrix = np.array(consumption_X_train_matrix)\n",
    "consumption_y_train_matrix = np.array(consumption_y_train_matrix)\n",
    "# reshape arrays\n",
    "consumption_X_train_reshaped = np.reshape(consumption_X_train_matrix, \n",
    "                                         (consumption_X_train_matrix.shape[0], \n",
    "                                          consumption_X_train_matrix.shape[1], 1))\n",
    "consumption_y_train_reshaped = np.reshape(consumption_y_train_matrix, \n",
    "                                         (consumption_y_train_matrix.shape[0], \n",
    "                                          consumption_y_train_matrix.shape[1], 1))\n",
    "\n",
    "# Repeat the same stes for generatin dataframe\n",
    "generation_X_train_matrix = np.array(generation_X_train_matrix)\n",
    "generation_y_train_matrix = np.array(generation_y_train_matrix)\n",
    "generation_X_train_reshaped = np.reshape(generation_X_train_matrix, \n",
    "                                         (generation_X_train_matrix.shape[0], \n",
    "                                          generation_X_train_matrix.shape[1], 1))\n",
    "generation_y_train_reshaped = np.reshape(generation_y_train_matrix, \n",
    "                                         (generation_y_train_matrix.shape[0], \n",
    "                                          generation_y_train_matrix.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train sets shape: (43392, 168, 1)\n",
      "y_train sets shape: (43392, 168, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "print()\n",
    "print('X_train sets shape:', generation_X_train_reshaped.shape)\n",
    "print('y_train sets shape:', generation_y_train_reshaped.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (128, 168, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (128, 168, 12)            672       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (128, 168, 12)            1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, 168, 1)             13        \n",
      "=================================================================\n",
      "Total params: 1,885\n",
      "Trainable params: 1,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising the LSTM Model with MSE Loss-Function\n",
    "# Using Functional API, each layer output is the input of next layer\n",
    "\n",
    "# Input\n",
    "inputs = Input(batch_shape=(batch_size,timesteps,1))\n",
    "\n",
    "# Layer 1: LSTM \n",
    "lstm_1 = LSTM(12, \n",
    "                activation='tanh', \n",
    "                recurrent_activation='sigmoid', \n",
    "                stateful=True, \n",
    "                return_sequences=True)(inputs)\n",
    "# Layer 2: LSTM \n",
    "lstm_2 = LSTM(12, \n",
    "                activation='tanh', \n",
    "                recurrent_activation='sigmoid', \n",
    "                stateful=True, \n",
    "                return_sequences=True)(lstm_1)\n",
    "# Output\n",
    "output = Dense(units = 1)(lstm_2)\n",
    "\n",
    "# Sticking all layers into a Model\n",
    "regressor = Model(inputs=inputs, outputs = output)\n",
    "\n",
    "#adam is fast starting off and then gets slower and more precise\n",
    "regressor.compile(optimizer='adam', loss = mean_squared_error)\n",
    "\n",
    "# Check the model summary\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the code on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Epoch 1/1\n",
      "43392/43392 [==============================] - 367s 8ms/step - loss: 0.0146\n",
      "\n",
      "Epoch: 1\n",
      "Epoch 1/1\n",
      "29952/43392 [===================>..........] - ETA: 1:51 - loss: 0.0109"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "# start time\n",
    "start=time.time()\n",
    "\n",
    "#Statefull\n",
    "for i in range(epochs):\n",
    "    print(\"\\nEpoch: \" + str(i))\n",
    "    #run through all data but the cell, hidden state are used for the next batch.\n",
    "    regressor.fit(consumption_X_train_reshaped, consumption_y_train_reshaped, \n",
    "                  shuffle=False, epochs = 1, batch_size = batch_size)\n",
    "    #resets only the states but the weights, cell and hidden are kept.\n",
    "    regressor.reset_states()\n",
    "\n",
    "# duration of training the model\n",
    "duration=time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.save(filepath=\"LSTM_Model_Consumption_128.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials= {\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceId-af7f118d-33e5-4db8-8af0-91ccdc9d6664',\n",
    "    'IBM_API_KEY_ID': 'MSokvXUNblLCMbjLQdSUSZjz69jMdutBIqI6S8JZeFkh',\n",
    "    'ENDPOINT': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'def-donotdelete-pr-rwrregjlstqzff',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file wine.csv from wine folder into project bucket as wine_data.csv\n",
    "cos.upload_file(Filename='LSTM_Model_Consumption_720.h5',Bucket=credentials['BUCKET'],Key='LSTM_Model_Consumption_720.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
